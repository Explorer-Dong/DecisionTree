{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗前的数据规模：训练集 -> (32561, 15) 测试集 -> (16281, 15)\n",
      "清洗后的数据规模：训练集 -> (30162, 15) 测试集 -> (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'educationNum', \n",
    "           'maritalStatus', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capitalGain', 'capitalLoss', 'hoursPerWeek', 'nativeCountry', 'income']\n",
    "continues_columns = ['age', 'fnlwgt', 'educationNum', 'capitalGain', 'capitalLoss', 'hoursPerWeek']\n",
    "discrete_columns = [_ for _ in columns if _ not in continues_columns]\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "train_data = pd.read_csv('data/adult.data', delimiter=', ', header=None, engine='python')\n",
    "test_data = pd.read_csv('data/adult.test', delimiter=', ', header=None, skiprows=1, engine='python')\n",
    "train_data.columns = columns\n",
    "test_data.columns = columns\n",
    "\n",
    "\n",
    "# 清洗数据：删除含有缺失值的样本\n",
    "print(f\"清洗前的数据规模：训练集 -> {train_data.shape} 测试集 -> {test_data.shape}\")\n",
    "for column in columns:\n",
    "    train_data = train_data[~train_data[column].astype(str).str.contains('\\?')]\n",
    "    test_data = test_data[~test_data[column].astype(str).str.contains('\\?')]\n",
    "print(f\"清洗后的数据规模：训练集 -> {train_data.shape} 测试集 -> {test_data.shape}\")\n",
    "\n",
    "\n",
    "# 离散数据：按列编码为float类型\n",
    "LE = LabelEncoder()\n",
    "for i in range(len(columns)):\n",
    "    if columns[i] in continues_columns: continue\n",
    "    train_data[columns[i]] = LE.fit_transform(train_data[columns[i]]).astype(float)\n",
    "    test_data[columns[i]] = LE.fit_transform(test_data[columns[i]]).astype(float)\n",
    "\n",
    "\n",
    "# 连续数据：离散化为分位数区间的中值\n",
    "for column in continues_columns:\n",
    "    train_qcut = pd.qcut(train_data[column].astype(float), 5, duplicates='drop')\n",
    "    train_data[column] = train_qcut.apply(lambda x: x.mid)\n",
    "    test_qcut = pd.qcut(test_data[column].astype(float), 5, duplicates='drop')\n",
    "    test_data[column] = test_qcut.apply(lambda x: x.mid)\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗前的数据规模：训练集 -> (32561, 15) 测试集 -> (16281, 15)\n",
      "清洗后的数据规模：训练集 -> (30162, 15) 测试集 -> (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'educationNum', \n",
    "           'maritalStatus', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capitalGain', 'capitalLoss', 'hoursPerWeek', 'nativeCountry', 'income']\n",
    "continues_columns = ['age', 'fnlwgt', 'educationNum', 'capitalGain', 'capitalLoss', 'hoursPerWeek']\n",
    "discrete_columns = [_ for _ in columns if _ not in continues_columns]\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "train_data = np.genfromtxt('data/adult.data', delimiter=', ', dtype=str, usecols=range(len(columns)), missing_values='?', filling_values=np.nan)\n",
    "test_data = np.genfromtxt('data/adult.test', delimiter=', ', dtype=str, usecols=range(len(columns)), skip_header=1, missing_values='?', filling_values=np.nan)\n",
    "\n",
    "\n",
    "# 清洗数据：删除含有缺失值的样本\n",
    "print(f\"清洗前的数据规模：训练集 -> {train_data.shape} 测试集 -> {test_data.shape}\")\n",
    "train_data = np.array([row for row in train_data if '?' not in row])\n",
    "test_data = np.array([row for row in test_data if '?' not in row])\n",
    "print(f\"清洗后的数据规模：训练集 -> {train_data.shape} 测试集 -> {test_data.shape}\")\n",
    "\n",
    "\n",
    "# 离散数据：按列编码为float类型\n",
    "unique_values = {}\n",
    "for i in range(len(columns)):\n",
    "    if columns[i] not in continues_columns:\n",
    "        unique_values[columns[i]] = np.unique(np.concatenate((train_data[:, i], test_data[:, i])))\n",
    "for i in range(len(columns)):\n",
    "    if columns[i] not in continues_columns:\n",
    "        train_data[:, i] = np.searchsorted(unique_values[columns[i]], train_data[:, i]).astype(float)\n",
    "        test_data[:, i] = np.searchsorted(unique_values[columns[i]], test_data[:, i]).astype(float)\n",
    "\n",
    "\n",
    "def quantile_discretization(data, quantiles):\n",
    "    sorted_data = np.sort(data)\n",
    "    quantile_values = np.percentile(sorted_data, quantiles)\n",
    "    discretized_data = np.zeros(data.shape, dtype=float)\n",
    "    for i in range(len(quantiles) - 1):\n",
    "        discretized_data[(data >= quantile_values[i]) & (data < quantile_values[i + 1])] = i\n",
    "    discretized_data[data >= quantile_values[-1]] = len(quantiles) - 1\n",
    "    return discretized_data\n",
    "\n",
    "# 连续数据：离散化为分位数区间的中值\n",
    "for column in continues_columns:\n",
    "    train_data[:, columns.index(column)] = quantile_discretization(\n",
    "        train_data[:, columns.index(column)].astype(float),\n",
    "        [20, 40, 60, 80]\n",
    "    )\n",
    "    \n",
    "    test_data[:, columns.index(column)] = quantile_discretization(\n",
    "        test_data[:, columns.index(column)].astype(float),\n",
    "        [20, 40, 60, 80]\n",
    "    )\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "X_train = train_data[:, :-1].astype(float)\n",
    "y_train = train_data[:, -1].astype(float)\n",
    "X_test = test_data[:, :-1].astype(float)\n",
    "y_test = test_data[:, -1].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、自定义决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attribute=None, children=None, label=None):\n",
    "        self.attribute = attribute  # 当前结点属性\n",
    "        self.children = children    # 子节点字典，value: Node\n",
    "        self.label = label          # 叶结点标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, criterion='entropy'):\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.tree = self._build_tree(X_train, y_train)\n",
    "\n",
    "\n",
    "    def _build_tree(self, X: pd.DataFrame, y: pd.Series):\n",
    "        # 如果数据集为空，返回 None\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "\n",
    "        # 如果标签相同，返回叶子结点\n",
    "        if len(set(y)) == 1:\n",
    "            return Node(label=y.iloc[0])\n",
    "\n",
    "        # 选择最佳划分属性\n",
    "        best_attribute, best_gain = self._choose_best_attribute(X, y)\n",
    "\n",
    "        # 如果没有合适的划分，返回叶子结点\n",
    "        if best_gain == 0:\n",
    "            return Node(label=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "        # 构建叶子结点\n",
    "        children = {}\n",
    "        for value in X[best_attribute].unique():\n",
    "            # 根据属性值划分数据集\n",
    "            value_indices = (X[best_attribute] == value)\n",
    "            X_subset = X.loc[value_indices]\n",
    "            y_subset = y.loc[value_indices]\n",
    "\n",
    "            # 递归构建叶子结点\n",
    "            child_node = self._build_tree(X_subset, y_subset)\n",
    "            children[value] = child_node\n",
    "\n",
    "        # 返回内叶子结点\n",
    "        return Node(attribute=best_attribute, children=children)\n",
    "\n",
    "\n",
    "    def _choose_best_attribute(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_attribute = None\n",
    "\n",
    "        # 遍历所有特征\n",
    "        for attribute in X.columns:\n",
    "            # 获取当前特征的唯一值\n",
    "            values = X[attribute].unique()\n",
    "\n",
    "            # 划分数据集\n",
    "            subsets = defaultdict(list)\n",
    "            for value in values:\n",
    "                indices = (X[attribute] == value)\n",
    "                y_subset = y.loc[indices]\n",
    "                subsets[value] = y_subset\n",
    "\n",
    "            gain = self._calculate_gain(y, subsets)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_attribute = attribute\n",
    "\n",
    "        return best_attribute, best_gain\n",
    "\n",
    "\n",
    "    def _calculate_gain(self, y, subsets):\n",
    "        # 根据选择的标准计算增益\n",
    "        if self.criterion == 'entropy':\n",
    "            gain = self._entropy_gain(y, subsets)\n",
    "        elif self.criterion == 'log_loss':\n",
    "            gain = self._ratio_gain(y, subsets)\n",
    "        elif self.criterion == 'gini':\n",
    "            gain = self._gini_gain(y, subsets)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported criterion\")\n",
    "        return gain\n",
    "\n",
    "\n",
    "    def _calculate_entropy(self, y: pd.Series) -> float:\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        entropy = 0\n",
    "\n",
    "        for count in counts.values():\n",
    "            prob = count / total\n",
    "            entropy -= prob * np.log2(prob)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def _entropy_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        entropy = self._calculate_entropy(y)\n",
    "        child_entropy = np.sum([len(subset) / len(y) * self._calculate_entropy(subset) for subset in subsets.values()])        \n",
    "        return entropy - child_entropy\n",
    "\n",
    "\n",
    "    def _ratio_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        gain = self._entropy_gain(y, subsets)\n",
    "        iv = np.sum([-len(subset) / len(y) * np.log2(len(subset) / len(y)) for subset in subsets.values()])\n",
    "        return 0 if iv == 0 else gain / iv\n",
    "    \n",
    "\n",
    "    def _gini_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        gini_index = 0\n",
    "        for subset in subsets.values():\n",
    "            gini_index += len(subset) / len(y) * (1 - np.sum([(len(subset.loc[subset == label]) / len(subset)) ** 2 for label in subset.unique()]))\n",
    "        return -gini_index\n",
    "\n",
    "\n",
    "    def predict(self, X_test) -> np.array:\n",
    "        y_pred = [self._predict_single_sample(sample) for _, sample in X_test.iterrows()]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "    def _predict_single_sample(self, sample):\n",
    "        node = self.tree\n",
    "        while node.label is None:\n",
    "            value = sample[node.attribute]\n",
    "\n",
    "            # 如果叶子结点为空，返回子结点中最多的标签\n",
    "            if value not in node.children or node.children[value] is None:\n",
    "                child_labels = []\n",
    "\n",
    "                # 仅当子节点存在且是叶节点时，获取其标签\n",
    "                for child_node in node.children.values():\n",
    "                    if child_node is not None and child_node.label is not None:\n",
    "                        child_labels.append(child_node.label)\n",
    "                \n",
    "                if len(child_labels) == 0: return 0.0\n",
    "                most_common_label = Counter(child_labels).most_common(1)[0][0]\n",
    "                return most_common_label\n",
    "\n",
    "            else:\n",
    "                node = node.children[value]\n",
    "\n",
    "        return node.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attribute=None, children=None, label=None):\n",
    "        self.attribute = attribute  # 当前结点属性\n",
    "        self.children = children    # 子节点字典，value: Node\n",
    "        self.label = label          # 叶结点标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, criterion='entropy'):\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.tree = self._build_tree(X_train, y_train)\n",
    "\n",
    "\n",
    "    def _build_tree(self, X: pd.DataFrame, y: pd.Series):\n",
    "        # 如果数据集为空，返回 None\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "\n",
    "        # 如果标签相同，返回叶子结点\n",
    "        if len(set(y)) == 1:\n",
    "            return Node(label=y[0])\n",
    "\n",
    "        # 选择最佳划分属性\n",
    "        best_attribute, best_gain = self._choose_best_attribute(X, y)\n",
    "\n",
    "        # 如果没有合适的划分，返回叶子结点\n",
    "        if best_gain == 0:\n",
    "            return Node(label=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "        # 构建叶子结点\n",
    "        children = {}\n",
    "        for value in np.unique(X[:, columns.index(best_attribute)]):\n",
    "            # 根据属性值划分数据集\n",
    "            value_indices = (X[:, columns.index(best_attribute)] == value)\n",
    "            X_subset = X[value_indices]\n",
    "            y_subset = y[value_indices]\n",
    "\n",
    "            # 递归构建叶子结点\n",
    "            child_node = self._build_tree(X_subset, y_subset)\n",
    "            children[value] = child_node\n",
    "\n",
    "        # 返回内叶子结点\n",
    "        return Node(attribute=best_attribute, children=children)\n",
    "\n",
    "\n",
    "    def _choose_best_attribute(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_attribute = None\n",
    "\n",
    "        # 遍历所有特征\n",
    "        for attribute_index in range(X.shape[1]):\n",
    "            # 获取当前特征的唯一值\n",
    "            values = np.unique(X[:, columns.index(attribute_index)])\n",
    "\n",
    "            # 划分数据集\n",
    "            subsets = defaultdict(list)\n",
    "            for value in values:\n",
    "                indices = (X[:, columns.index(attribute_index)] == value)\n",
    "                y_subset = y[indices]\n",
    "                subsets[value] = y_subset\n",
    "\n",
    "            gain = self._calculate_gain(y, subsets)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_attribute = columns[attribute_index]\n",
    "\n",
    "        return best_attribute, best_gain\n",
    "\n",
    "\n",
    "    def _calculate_gain(self, y, subsets):\n",
    "        # 根据选择的标准计算增益\n",
    "        if self.criterion == 'entropy':\n",
    "            gain = self._entropy_gain(y, subsets)\n",
    "        elif self.criterion == 'log_loss':\n",
    "            gain = self._ratio_gain(y, subsets)\n",
    "        elif self.criterion == 'gini':\n",
    "            gain = self._gini_gain(y, subsets)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported criterion\")\n",
    "        return gain\n",
    "\n",
    "\n",
    "    def _calculate_entropy(self, y: pd.Series) -> float:\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        entropy = 0\n",
    "\n",
    "        for count in counts.values():\n",
    "            prob = count / total\n",
    "            entropy -= prob * np.log2(prob)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def _entropy_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        entropy = self._calculate_entropy(y)\n",
    "        child_entropy = np.sum([len(subset) / len(y) * self._calculate_entropy(subset) for subset in subsets.values()])        \n",
    "        return entropy - child_entropy\n",
    "\n",
    "\n",
    "    def _ratio_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        gain = self._entropy_gain(y, subsets)\n",
    "        iv = np.sum([-len(subset) / len(y) * np.log2(len(subset) / len(y)) for subset in subsets.values()])\n",
    "        return 0 if iv == 0 else gain / iv\n",
    "    \n",
    "\n",
    "    def _gini_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        gini_index = 0\n",
    "        for subset in subsets.values():\n",
    "            gini_index += len(subset) / len(y) * (1 - np.sum([(len(subset.loc[subset == label]) / len(subset)) ** 2 for label in subset.unique()]))\n",
    "        return -gini_index\n",
    "\n",
    "\n",
    "    def predict(self, X_test) -> np.array:\n",
    "        y_pred = [self._predict_single_sample(sample) for sample in X_test]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "    def _predict_single_sample(self, sample):\n",
    "        node = self.tree\n",
    "        while node.label is None:\n",
    "            value = sample[columns.index(node.attribute)]\n",
    "\n",
    "            # 如果叶子结点为空，返回子结点中最多的标签\n",
    "            if value not in node.children or node.children[value] is None:\n",
    "                child_labels = []\n",
    "\n",
    "                # 仅当子节点存在且是叶节点时，获取其标签\n",
    "                for child_node in node.children.values():\n",
    "                    if child_node is not None and child_node.label is not None:\n",
    "                        child_labels.append(child_node.label)\n",
    "                \n",
    "                if len(child_labels) == 0: return 0.0\n",
    "                most_common_label = Counter(child_labels).most_common(1)[0][0]\n",
    "                return most_common_label\n",
    "\n",
    "            else:\n",
    "                node = node.children[value]\n",
    "\n",
    "        return node.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、多模型对照测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 使用 entropy 作为划分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准包准确率：0.00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "0 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m标准包准确率：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m clf_my \u001b[38;5;241m=\u001b[39m MyDecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mclf_my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m y_pred_my \u001b[38;5;241m=\u001b[39m clf_my\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred_my)\n",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m, in \u001b[0;36mMyDecisionTreeClassifier.fit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m, in \u001b[0;36mMyDecisionTreeClassifier._build_tree\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(label\u001b[38;5;241m=\u001b[39my[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 选择最佳划分属性\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m best_attribute, best_gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_choose_best_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 如果没有合适的划分，返回叶子结点\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_gain \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[25], line 50\u001b[0m, in \u001b[0;36mMyDecisionTreeClassifier._choose_best_attribute\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 遍历所有特征\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attribute_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# 获取当前特征的唯一值\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(X[:, \u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute_index\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# 划分数据集\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     subsets \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: 0 is not in list"
     ]
    }
   ],
   "source": [
    "clf_std = DecisionTreeClassifier(criterion='entropy')\n",
    "clf_std.fit(X_train, y_train)\n",
    "y_pred_std = clf_std.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_std)\n",
    "print(f'标准包准确率：{accuracy:.2f}')\n",
    "\n",
    "clf_my = MyDecisionTreeClassifier(criterion='entropy')\n",
    "clf_my.fit(X_train, y_train)\n",
    "y_pred_my = clf_my.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_my)\n",
    "print(f'自实现准确率：{accuracy:.2f}')\n",
    "\n",
    "# tree.plot_tree(clf_std, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用 log_loss 作为划分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准包准确率：0.78\n",
      "自实现准确率：0.78\n"
     ]
    }
   ],
   "source": [
    "clf_std = DecisionTreeClassifier(criterion='log_loss')\n",
    "clf_std.fit(X_train, y_train)\n",
    "y_pred_std = clf_std.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_std)\n",
    "print(f'标准包准确率：{accuracy:.2f}')\n",
    "\n",
    "\n",
    "clf_my = MyDecisionTreeClassifier(criterion='log_loss')\n",
    "clf_my.fit(X_train, y_train)\n",
    "y_pred_my = clf_my.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_my)\n",
    "print(f'自实现准确率：{accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 使用 gini 作为划分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准包准确率：0.78\n",
      "自实现准确率：0.75\n"
     ]
    }
   ],
   "source": [
    "clf_std = DecisionTreeClassifier(criterion='gini')\n",
    "clf_std.fit(X_train, y_train)\n",
    "y_pred_std = clf_std.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_std)\n",
    "print(f'标准包准确率：{accuracy:.2f}')\n",
    "\n",
    "\n",
    "clf_my = MyDecisionTreeClassifier(criterion='gini')\n",
    "clf_my.fit(X_train, y_train)\n",
    "y_pred_my = clf_my.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_my)\n",
    "print(f'自实现准确率：{accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
