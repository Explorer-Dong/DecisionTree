{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗前的数据规模：训练集 -> (32561, 15) 测试集 -> (16281, 15)\n",
      "清洗后的数据规模：训练集 -> (30162, 15) 测试集 -> (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'educationNum', \n",
    "           'maritalStatus', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capitalGain', 'capitalLoss', 'hoursPerWeek', 'nativeCountry', 'income']\n",
    "continues_columns = ['age', 'fnlwgt', 'educationNum', 'capitalGain', 'capitalLoss', 'hoursPerWeek']\n",
    "discrete_columns = [_ for _ in columns if _ not in continues_columns]\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "train_data = pd.read_csv('data/adult.data', delimiter=', ', header=None, engine='python')\n",
    "test_data = pd.read_csv('data/adult.test', delimiter=', ', header=None, skiprows=1, engine='python')\n",
    "train_data.columns = columns\n",
    "test_data.columns = columns\n",
    "\n",
    "\n",
    "# 清洗数据：删除含有缺失值的样本\n",
    "print(f\"清洗前的数据规模：训练集 -> {train_data.shape} 测试集 -> {test_data.shape}\")\n",
    "for column in columns:\n",
    "    train_data = train_data[~train_data[column].astype(str).str.contains('\\?')]\n",
    "    test_data = test_data[~test_data[column].astype(str).str.contains('\\?')]\n",
    "print(f\"清洗后的数据规模：训练集 -> {train_data.shape} 测试集 -> {test_data.shape}\")\n",
    "\n",
    "\n",
    "# 离散数据：按列编码为float类型\n",
    "LE = LabelEncoder()\n",
    "for i in range(len(columns)):\n",
    "    if columns[i] in continues_columns: continue\n",
    "    train_data[columns[i]] = LE.fit_transform(train_data[columns[i]]).astype(float)\n",
    "    test_data[columns[i]] = LE.fit_transform(test_data[columns[i]]).astype(float)\n",
    "\n",
    "\n",
    "# 连续数据：离散化为分位数区间的中值\n",
    "for column in continues_columns:\n",
    "    train_qcut = pd.qcut(train_data[column].astype(float), 5, duplicates='drop')\n",
    "    train_data[column] = train_qcut.apply(lambda x: x.mid)\n",
    "    test_qcut = pd.qcut(test_data[column].astype(float), 5, duplicates='drop')\n",
    "    test_data[column] = test_qcut.apply(lambda x: x.mid)\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、自定义决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attribute=None, children=None, label=None):\n",
    "        self.attribute = attribute  # 当前结点属性\n",
    "        self.children = children    # 子节点字典，value: Node\n",
    "        self.label = label          # 叶结点标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, criterion='entropy'):\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.tree = self._build_tree(X_train, y_train)\n",
    "\n",
    "\n",
    "    def _build_tree(self, X: pd.DataFrame, y: pd.Series):\n",
    "        # 如果数据集为空，返回 None\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "\n",
    "        # 如果标签相同，返回叶子结点\n",
    "        if len(set(y)) == 1:\n",
    "            return Node(label=y.iloc[0])\n",
    "\n",
    "        # 选择最佳划分属性\n",
    "        best_attribute, best_gain = self._choose_best_attribute(X, y)\n",
    "\n",
    "        # 如果没有合适的划分，返回叶子结点\n",
    "        if best_gain == 0:\n",
    "            return Node(label=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "        # 构建叶子结点\n",
    "        children = {}\n",
    "        for value in X[best_attribute].unique():\n",
    "            # 根据属性值划分数据集\n",
    "            value_indices = (X[best_attribute] == value)\n",
    "            X_subset = X.loc[value_indices]\n",
    "            y_subset = y.loc[value_indices]\n",
    "\n",
    "            # 递归构建叶子结点\n",
    "            child_node = self._build_tree(X_subset, y_subset)\n",
    "            children[value] = child_node\n",
    "\n",
    "        # 返回内叶子结点\n",
    "        return Node(attribute=best_attribute, children=children)\n",
    "\n",
    "\n",
    "    def _choose_best_attribute(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_attribute = None\n",
    "\n",
    "        # 遍历所有特征\n",
    "        for attribute in X.columns:\n",
    "            # 获取当前特征的唯一值\n",
    "            values = X[attribute].unique()\n",
    "\n",
    "            # 划分数据集\n",
    "            subsets = defaultdict(list)\n",
    "            for value in values:\n",
    "                indices = (X[attribute] == value)\n",
    "                y_subset = y.loc[indices]\n",
    "                subsets[value] = y_subset\n",
    "\n",
    "            gain = self._calculate_gain(y, subsets)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_attribute = attribute\n",
    "\n",
    "        return best_attribute, best_gain\n",
    "\n",
    "\n",
    "    def _calculate_gain(self, y, subsets):\n",
    "        # 根据选择的标准计算增益\n",
    "        if self.criterion == 'entropy':\n",
    "            gain = self._entropy_gain(y, subsets)\n",
    "        elif self.criterion == 'log_loss':\n",
    "            gain = self._ratio_gain(y, subsets)\n",
    "        elif self.criterion == 'gini':\n",
    "            gain = self._gini_gain(y, subsets)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported criterion\")\n",
    "        return gain\n",
    "\n",
    "\n",
    "    def _calculate_entropy(self, y: pd.Series) -> float:\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        entropy = 0\n",
    "\n",
    "        for count in counts.values():\n",
    "            prob = count / total\n",
    "            entropy -= prob * np.log2(prob)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def _entropy_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        entropy = self._calculate_entropy(y)\n",
    "        child_entropy = np.sum([len(subset) / len(y) * self._calculate_entropy(subset) for subset in subsets.values()])        \n",
    "        return entropy - child_entropy\n",
    "\n",
    "\n",
    "    def _ratio_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        gain = self._entropy_gain(y, subsets)\n",
    "        iv = np.sum([-len(subset) / len(y) * np.log2(len(subset) / len(y)) for subset in subsets.values()])\n",
    "        return 0 if iv == 0 else gain / iv\n",
    "    \n",
    "\n",
    "    def _gini_gain(self, y: pd.Series, subsets: dict) -> float:\n",
    "        gini_index = 0\n",
    "        for subset in subsets.values():\n",
    "            gini_index += len(subset) / len(y) * (1 - np.sum([(len(subset.loc[subset == label]) / len(subset)) ** 2 for label in subset.unique()]))\n",
    "        return -gini_index\n",
    "\n",
    "\n",
    "    def predict(self, X_test) -> np.array:\n",
    "        y_pred = [self._predict_single_sample(sample) for _, sample in X_test.iterrows()]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "    def _predict_single_sample(self, sample):\n",
    "        node = self.tree\n",
    "        while node.label is None:\n",
    "            value = sample[node.attribute]\n",
    "\n",
    "            # 如果叶子结点为空，返回子结点中最多的标签\n",
    "            if value not in node.children or node.children[value] is None:\n",
    "                child_labels = []\n",
    "\n",
    "                # 仅当子节点存在且是叶节点时，获取其标签\n",
    "                for child_node in node.children.values():\n",
    "                    if child_node is not None and child_node.label is not None:\n",
    "                        child_labels.append(child_node.label)\n",
    "                \n",
    "                if len(child_labels) == 0: return 0.0\n",
    "                most_common_label = Counter(child_labels).most_common(1)[0][0]\n",
    "                return most_common_label\n",
    "\n",
    "            else:\n",
    "                node = node.children[value]\n",
    "\n",
    "        return node.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、多模型对照测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 使用 entropy 作为划分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准包准确率：0.78\n",
      "自实现准确率：0.77\n"
     ]
    }
   ],
   "source": [
    "clf_std = DecisionTreeClassifier(criterion='entropy')\n",
    "clf_std.fit(X_train, y_train)\n",
    "y_pred_std = clf_std.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_std)\n",
    "print(f'标准包准确率：{accuracy:.2f}')\n",
    "\n",
    "clf_my = MyDecisionTreeClassifier(criterion='entropy')\n",
    "clf_my.fit(X_train, y_train)\n",
    "y_pred_my = clf_my.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_my)\n",
    "print(f'自实现准确率：{accuracy:.2f}')\n",
    "\n",
    "# tree.plot_tree(clf_std, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用 log_loss 作为划分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准包准确率：0.78\n",
      "自实现准确率：0.78\n"
     ]
    }
   ],
   "source": [
    "clf_std = DecisionTreeClassifier(criterion='log_loss')\n",
    "clf_std.fit(X_train, y_train)\n",
    "y_pred_std = clf_std.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_std)\n",
    "print(f'标准包准确率：{accuracy:.2f}')\n",
    "\n",
    "\n",
    "clf_my = MyDecisionTreeClassifier(criterion='log_loss')\n",
    "clf_my.fit(X_train, y_train)\n",
    "y_pred_my = clf_my.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_my)\n",
    "print(f'自实现准确率：{accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 使用 gini 作为划分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准包准确率：0.78\n",
      "自实现准确率：0.75\n"
     ]
    }
   ],
   "source": [
    "clf_std = DecisionTreeClassifier(criterion='gini')\n",
    "clf_std.fit(X_train, y_train)\n",
    "y_pred_std = clf_std.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_std)\n",
    "print(f'标准包准确率：{accuracy:.2f}')\n",
    "\n",
    "\n",
    "clf_my = MyDecisionTreeClassifier(criterion='gini')\n",
    "clf_my.fit(X_train, y_train)\n",
    "y_pred_my = clf_my.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_my)\n",
    "print(f'自实现准确率：{accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
